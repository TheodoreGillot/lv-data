{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import unidecode\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords and configure if not already available\n",
    "nltk.download('stopwords')\n",
    "\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "custom_stopwords = {\n",
    "    \"des\", \"dun\", \"cette\", \"pour\", \"dune\", \"un\", \"deux\", \"trois\", \"quatre\", \"louis\", \"vuitton\", \"lv\", \"modle\", \"fermeture\"       \n",
    "}\n",
    "\n",
    "french_stopwords.update(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with special characters\n",
    "def remove_accents(text):\n",
    "    \"\"\"\n",
    "    Convert accented characters to their unaccented counterparts.\n",
    "    E.g., 'é' -> 'e', 'à' -> 'a', 'œ' -> 'oe'\n",
    "    \"\"\"\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading configuration and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../cfg/config.ini')\n",
    "\n",
    "data_dir = config['DEFAULT'].get('data_dir')\n",
    "\n",
    "product_df_path = os.path.join(data_dir, config['DEFAULT'].get('combined_product', fallback=None))\n",
    "client_df_path = os.path.join(data_dir, config['DEFAULT'].get('combined_client'))\n",
    "transac_df_path = os.path.join(data_dir, config['DEFAULT'].get('combined_transac'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The separators may differ by file as in your config snippet\n",
    "product = pd.read_csv(product_df_path, sep=',')\n",
    "client = pd.read_csv(client_df_path, sep=';')\n",
    "transac = pd.read_csv(transac_df_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing product descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean HTML tags from sku_description using BeautifulSoup\n",
    "def clean_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Create a clean text column (handle potential missing values)\n",
    "product['clean_description'] = product['sku_description'].apply(lambda x: clean_html(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Tokenization and cleaning\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = remove_accents(text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    tokens = [w for w in words if w not in french_stopwords and len(w) > 2]\n",
    "    return tokens\n",
    "\n",
    "product['tokens'] = product['clean_description'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall frequency in product descriptions\n",
    "all_words = [word for tokens in product['tokens'] for word in tokens]\n",
    "word_counts = Counter(all_words)\n",
    "top_words = word_counts.most_common(20)\n",
    "print(\"Top words overall in product descriptions:\")\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted frequency using transaction data\n",
    "# Merge product and transac on product_id to get sales counts for each product\n",
    "product_transac = pd.merge(product[['product_id', 'tokens']], transac[['product_id', 'product_quantity']], on='product_id', how='inner')\n",
    "\n",
    "# Each word is weighted by the number of items sold (product_quantity)\n",
    "weighted_counter = Counter()\n",
    "for _, row in product_transac.iterrows():\n",
    "    qty = row['product_quantity']\n",
    "    for word in row['tokens']:\n",
    "        weighted_counter[word] += qty\n",
    "\n",
    "top_weighted_words = weighted_counter.most_common(20)\n",
    "print(\"Top weighted words (by sales volume):\")\n",
    "print(top_weighted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend analysis and forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert week strings (e.g., \"W202310\") to a datetime.\n",
    "\n",
    "def week_to_date(week_str):\n",
    "    year = int(week_str[1:5])\n",
    "    week = int(week_str[5:])\n",
    "    return datetime.strptime(f'{year}-W{week}-1', \"%Y-W%W-%w\")\n",
    "\n",
    "transac['week_date'] = transac['week'].apply(week_to_date)\n",
    "\n",
    "# For a subset of top words (e.g., top 5), track weekly sales trends.\n",
    "top_words_list = [word for word, count in top_weighted_words[:5]]\n",
    "trend_data = {}\n",
    "\n",
    "for word in top_words_list:\n",
    "    # Get product_ids where the description tokens contain the word\n",
    "    product_ids_with_word = product[product['tokens'].apply(lambda tokens: word in tokens)]['product_id'].unique()\n",
    "    # Filter transactions for these product_ids\n",
    "    df_word = transac[transac['product_id'].isin(product_ids_with_word)]\n",
    "    # Aggregate total sold per week\n",
    "    sales_by_week = df_word.groupby('week_date')['product_quantity'].sum().sort_index()\n",
    "    trend_data[word] = sales_by_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear forecasting for the next 4 weeks based on historical data\n",
    "\n",
    "forecast_results = {}\n",
    "for word, series in trend_data.items():\n",
    "    if len(series) < 5:\n",
    "        continue  # Skip if there isn’t enough data to forecast\n",
    "    # Create an ordinal time index\n",
    "    x = np.arange(len(series))\n",
    "    y = series.values\n",
    "    # Fit a linear model\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    forecast_x = np.arange(len(series), len(series) + 4)\n",
    "    forecast_y = intercept + slope * forecast_x\n",
    "    forecast_results[word] = forecast_y\n",
    "    \n",
    "    # Plot actual sales and forecast\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(series.index, y, marker='o', label='Actual Sales')\n",
    "    # Generate forecast dates using the interval between weeks\n",
    "    interval = series.index[1] - series.index[0]\n",
    "    forecast_dates = [series.index[-1] + interval * (i + 1) for i in range(4)]\n",
    "    plt.plot(forecast_dates, forecast_y, marker='x', linestyle='--', label='Forecast')\n",
    "    plt.title(f\"Sales Trend and Forecast for '{word}'\")\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Total Quantity Sold\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and forecast summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forecast predictions for the top words:\")\n",
    "for word, forecast in forecast_results.items():\n",
    "    print(f\"Word: '{word}', Forecast for next 4 weeks: {forecast}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
