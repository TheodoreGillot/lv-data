{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets using the read_csv() method. You can specify the separator used in the csv file. By default, the separator is \",\"\n",
    "products_df = pd.read_csv('product_inf_2000.csv')\n",
    "client_df = pd.read_csv('client_inf_2000.csv',sep=';')\n",
    "transactions_df = pd.read_csv('transac_inf_2000.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('product(1).csv')\n",
    "client = pd.read_csv('client(1).csv', sep=';')\n",
    "transactions = pd.read_csv('transac(1).csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des DataFrames\n",
    "combined_products = pd.concat([products, products_df], ignore_index=True)\n",
    "\n",
    "# Harmonisation des types de colonnes selon le plus grand DataFrame\n",
    "for col in products_df.columns:\n",
    "    combined_products[col] = combined_products[col].astype(products_df[col].dtype)\n",
    "\n",
    "# Vérification des informations du DataFrame combiné\n",
    "print(combined_products.info())\n",
    "\n",
    "# Optionnel : sauvegarde en CSV\n",
    "combined_products.to_csv('combined_products.csv', index=False, sep=',', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des DataFrames\n",
    "combined_client = pd.concat([client, client_df], ignore_index=True)\n",
    "\n",
    "# Harmonisation des types de colonnes selon client_df (le plus grand DataFrame)\n",
    "for col in client_df.columns:\n",
    "    combined_client[col] = combined_client[col].astype(client_df[col].dtype)\n",
    "\n",
    "# Vérification des informations du DataFrame combiné\n",
    "print(combined_client.info())\n",
    "\n",
    "# Optionnel : sauvegarde en CSV\n",
    "combined_client.to_csv('combined_client.csv', index=False, sep=',', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne 'website_version' en 'country' pour uniformiser\n",
    "transactions_df = transactions_df.rename(columns={'website_version': 'country'})\n",
    "\n",
    "# Concaténation\n",
    "combined_transactions = pd.concat([transactions, transactions_df], ignore_index=True)\n",
    "\n",
    "# Harmonisation des types selon transactions_df\n",
    "for col in transactions_df.columns:\n",
    "    combined_transactions[col] = combined_transactions[col].astype(transactions_df[col].dtype)\n",
    "\n",
    "# Vérification\n",
    "print(combined_transactions.info())\n",
    "\n",
    "# Optionnel : sauvegarde en CSV\n",
    "combined_transactions.to_csv('combined_transactions.csv', index=False, sep=',', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_and_year(week_string):\n",
    "    # Extract the year and week number from the input string\n",
    "    year_str = week_string[1:5]\n",
    "    week_num_str = week_string[5:]\n",
    "\n",
    "    # Convert the year and week number to integers\n",
    "    year = int(year_str)\n",
    "    week_num = int(week_num_str)\n",
    "\n",
    "    # Extract the month and year of every day of the week and chose the average month and year\n",
    "    months, years = [], []\n",
    "    try:\n",
    "            # Handle week 0 (January of the given year)\n",
    "            if week_num == 0:\n",
    "                day = datetime.date.fromisocalendar(year, 1, 1)  # First day of the year\n",
    "                months.append(day.month)\n",
    "                years.append(day.year)\n",
    "    \n",
    "            # Handle valid week numbers (1 to 52, or 53 if valid)\n",
    "            else:\n",
    "                for i in range(1, 8):  # Iterate over the days of the week\n",
    "                    day = datetime.date.fromisocalendar(year, week_num, i)\n",
    "                    months.append(day.month)\n",
    "                    years.append(day.year)\n",
    "    \n",
    "    except ValueError:\n",
    "            # Handle invalid week numbers (like non-existent week 53)\n",
    "            last_day_of_year = datetime.date(year, 12, 31)\n",
    "            months.append(last_day_of_year.month)\n",
    "            years.append(last_day_of_year.year)\n",
    "        \n",
    "    average_month = Counter(months).most_common(1)[0][0]\n",
    "    average_year = Counter(years).most_common(1)[0][0]\n",
    "\n",
    "    # Convert the month to the corresponding label\n",
    "    month = calendar.month_name[average_month]\n",
    "\n",
    "    return month[:3] + '-' + str(average_year)\n",
    "\n",
    "#Example\n",
    "week_string = \"W202148\"\n",
    "print(get_month_and_year(week_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_client['week'] = combined_client['week'].apply(get_month_and_year)\n",
    "combined_client.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_transactions['week'] = combined_transactions['week'].apply(get_month_and_year)\n",
    "combined_transactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#D4C4B7',    # beige\n",
    "          '#8B4513',    # saddle brown\n",
    "          '#2F1810']    # dark brown/black\n",
    "\n",
    "sales_by_store = combined_transactions.groupby(['week', 'store_type_label'])['product_quantity'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=sales_by_store, \n",
    "            x='week', \n",
    "            y='product_quantity', \n",
    "            hue='store_type_label',\n",
    "            palette=colors)\n",
    "\n",
    "plt.title('Évolution des ventes Web vs Magasins Physiques', fontsize=12, pad=15)\n",
    "plt.xlabel('Semaine', fontsize=10)\n",
    "plt.ylabel('Quantité de produits vendus', fontsize=10)\n",
    "\n",
    "# Rotate x-axis labels 45 degrees\n",
    "plt.xticks(rotation=45, ha='right')  # ha='right' aligns the rotated labels\n",
    "\n",
    "plt.legend(title='Type de magasin', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.2)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a mask for Web sales\n",
    "web_sales = combined_transactions[combined_transactions['store_type_label'] == 'Web']\n",
    "\n",
    "# Group by week for Web sales\n",
    "sales_by_store = web_sales.groupby('week')['product_quantity'].sum().reset_index()\n",
    "\n",
    "# Create the visualization\n",
    "colors = ['#8B4513']    # Using brown for web sales\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=sales_by_store, \n",
    "            x='week', \n",
    "            y='product_quantity',\n",
    "            color=colors[0],\n",
    "            linewidth=2)\n",
    "\n",
    "plt.title('Évolution des ventes Web', fontsize=12, pad=15)\n",
    "plt.xlabel('Semaine', fontsize=10)\n",
    "plt.ylabel('Quantité de produits vendus', fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.gca().set_facecolor('#FAF9F6') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_models.py\n",
    "class ClusteringModel:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def prepare_clustering_data(self, combined_client, combined_transactions, combined_products):\n",
    "    \n",
    "    # Agrégation des transactions par client\n",
    "        client_purchase_history = combined_transactions.groupby('week').agg({\n",
    "        'count_distinct_transaction': 'sum',\n",
    "        'product_quantity': 'sum'\n",
    "        }).reset_index()\n",
    "    \n",
    "    # Calcul des variables pour le clustering\n",
    "        client_features = pd.DataFrame()\n",
    "        client_features['total_transactions'] = combined_client.groupby('clients')['items_bought'].sum()\n",
    "        client_features['frequency'] = combined_client.groupby('clients')['week'].nunique()\n",
    "    \n",
    "    # Gestion des valeurs manquantes\n",
    "        client_features = client_features.fillna(0)\n",
    "    \n",
    "    # Standardisation des features\n",
    "        features_scaled = self.scaler.fit_transform(client_features)\n",
    "        return pd.DataFrame(features_scaled, columns=client_features.columns)\n",
    "\n",
    "    def visualize_clusters(self, data, clusters, method='kmeans'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=clusters, cmap='viridis')\n",
    "        plt.title(f'Visualisation des clusters ({method})')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans_model.py\n",
    "class KMeansModel(ClusteringModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kmeans = None\n",
    "        \n",
    "    def cluster(self, data, n_clusters=3):\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=1234)\n",
    "        clusters = self.kmeans.fit_predict(data)\n",
    "        \n",
    "        silhouette_avg = silhouette_score(data, clusters)\n",
    "        print(f\"Score silhouette K-means: {silhouette_avg}\")\n",
    "        \n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANModel(ClusteringModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dbscan = None\n",
    "    \n",
    "    def cluster(self, data, eps=0.3, min_samples=3): \n",
    "    \n",
    "        self.dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clusters = self.dbscan.fit_predict(data)\n",
    "        \n",
    "        n_clusters = len(set(clusters[clusters != -1]))\n",
    "        \n",
    "        print(f\"Number of clusters found: {n_clusters}\")\n",
    "        print(f\"Number of noise points: {sum(clusters == -1)}\")\n",
    "        \n",
    "        mask = clusters != -1\n",
    "        if n_clusters >= 2 and mask.any():\n",
    "            try:\n",
    "                silhouette_avg = silhouette_score(data[mask], clusters[mask])\n",
    "                print(f\"Score silhouette DBSCAN: {silhouette_avg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate silhouette score: {str(e)}\")\n",
    "        else:\n",
    "            print(\"Not enough clusters found for silhouette score calculation\")\n",
    "            if eps > 0.1:\n",
    "                print(\"Attempting with smaller eps...\")\n",
    "                return self.cluster(data, eps=eps/2, min_samples=min_samples)\n",
    "        \n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_transactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_from_transaction_data(combined_transactions,  'week', 'product_quantity').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clv_models.py\n",
    "class CLVModel:\n",
    "    def prepare_clv_data(self, combined_transactions):\n",
    "        return summary_data_from_transaction_data(\n",
    "            combined_transactions,\n",
    "            'clients',\n",
    "            'week',\n",
    "            'product_quantity',\n",
    "            'count_distinct_transaction'\n",
    "        )\n",
    "\n",
    "    def visualize_clv_distribution(self, predicted_clv):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(predicted_clv, bins=50)\n",
    "        plt.title('Distribution des CLV prédits')\n",
    "        plt.xlabel('CLV prédit')\n",
    "        plt.ylabel('Fréquence')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bgnbd_model.py\n",
    "class BGNBDModel(CLVModel):\n",
    "    def __init__(self):\n",
    "        self.bgf = None\n",
    "        \n",
    "    def fit(self, summary_data):\n",
    "        self.bgf = BetaGeoFitter(penalizer_coef=0.0)\n",
    "        self.bgf.fit(\n",
    "            summary_data['frequency'],\n",
    "            summary_data['recency'],\n",
    "            summary_data['T']\n",
    "        )\n",
    "        \n",
    "        predicted_purchases = self.bgf.predict(\n",
    "            summary_data['frequency'],\n",
    "            summary_data['recency'],\n",
    "            summary_data['T'],\n",
    "            30\n",
    "        )\n",
    "        mse = np.mean((predicted_purchases - summary_data['frequency']) ** 2)\n",
    "        print(f\"MSE du modèle BG/NBD: {mse}\")\n",
    "        \n",
    "        return predicted_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# gamma_gamma_model.py\n",
    "class GammaGammaModel(CLVModel):\n",
    "    def __init__(self):\n",
    "        self.ggf = None\n",
    "        \n",
    "    def fit(self, summary_data):\n",
    "        self.ggf = GammaGammaFitter(penalizer_coef=0.0)\n",
    "        self.ggf.fit(\n",
    "            summary_data['frequency'],\n",
    "            summary_data['monetary_value']\n",
    "        )\n",
    "        \n",
    "        predicted_clv = self.ggf.conditional_expected_average_profit(\n",
    "            summary_data['frequency'],\n",
    "            summary_data['monetary_value']\n",
    "        )\n",
    "        mse = np.mean((predicted_clv - summary_data['monetary_value']) ** 2)\n",
    "        print(f\"MSE du modèle Gamma-Gamma: {mse}\")\n",
    "        \n",
    "        return predicted_clv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "def main():\n",
    "    # Chargement des données\n",
    "    client_data = pd.read_csv('combined_client.csv')\n",
    "    transaction_data = pd.read_csv('combined_transactions.csv')\n",
    "    product_data = pd.read_csv('combined_products.csv')\n",
    "    \n",
    "    # Clustering avec KMeans\n",
    "    kmeans = KMeansModel()\n",
    "    clustering_data = kmeans.prepare_clustering_data(client_data, transaction_data, product_data)\n",
    "    kmeans_clusters = kmeans.cluster(clustering_data)\n",
    "    kmeans.visualize_clusters(clustering_data, kmeans_clusters, 'kmeans')\n",
    "    \n",
    "    # Clustering avec DBSCAN\n",
    "    dbscan = DBSCANModel()\n",
    "    dbscan_clusters = dbscan.cluster(clustering_data)\n",
    "    dbscan.visualize_clusters(clustering_data, dbscan_clusters, 'dbscan')\n",
    "    \n",
    "    # Analyse CLV avec BG/NBD\n",
    "    bgnbd = BGNBDModel()\n",
    "    clv_data = bgnbd.prepare_clv_data(transaction_data)\n",
    "    predicted_frequency = bgnbd.fit(clv_data)\n",
    "    \n",
    "    # Analyse CLV avec Gamma-Gamma\n",
    "    gamma = GammaGammaModel()\n",
    "    predicted_clv = gamma.fit(clv_data)\n",
    "    gamma.visualize_clv_distribution(predicted_clv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
